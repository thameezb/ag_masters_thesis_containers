\chapter{Discussion, Conclusion and Future Work}
\label{sec:discussion}

This chapter evaluates the aforementioned results in context of the posed problem.
Additionally concluding statements are presented, with potential future-work mentioned.

\section{Discussion}

\subsection{Performance and Latency}
As illustrated in the previous chapter, a near constant pattern can be seen across each of the benchmarks run. \textit{EKS} workloads performed closest (and occasionally outperformed) the baseline \textit{RKE} instances,
with \textit{ECS} workloads lagging behind. This can be attributed to the hidden overhead by the managing agents and platform architecture of \textit{ECS}.
Being a \emph{proprietary} and \emph{closed} technology, all performance optimization is done \emph{in-house} by the cloud-provider.
Comparing this to \textit{Kubernetes}, being an \textit{open-source} technology, it is worked on by over 43 000 (as of 2020)\cite{pittman_2020} of contributors, which would contribute towards its consistently better optimized
performance. This would be applicable to features like burst-able resource limits (which allows a container instance to consume above its resource request for short-periods of time) as well.

Additionally it is noted that \textit{Serverless} (\textit{fargate} and \textit{Lambda})
architectures showed significant performance drops when compared to their \textit{Virtual Machine} (EC2) counterparts,
even when managed by the same orchestration platform.
A potential reasoning for this would be the extra network-hops required for these \textit{Serverless}
instances to communicate with the platform managing-agents and other resources within a customer \textit{VPC}.
Finally, \textit{Serverless} platforms run on shared tenant hardware managed by the cloud provider,
which would in turn be stricter on the resource usage per container instance.

\subsection{Cost}
For short lived or burst-able workloads, \textit{Serverless} options have the clear advantage as there are no additional environmental costs to consider.
However, as noted by the results of forecast in Table\ref{fig:cost_projected}, the longer the workload needs to run, the more cost-effective \textit{EC2} instances become.
Whilst \textit{ECS} and \textit{EKS} cost the same from a pure workload perspective, the additional monthly cost of running an \textit{EKS} cluster does become a contributing factor
when comparing platforms at a pure dollar cost.

\subsection{Resilience and Reliability}
\textit{EC2} instances have the distinct speed advantage when restarting unhealthy containers.
This can be attributed to the container images already being present on the nodes required, whereas \textit{fargate} starts a purely distinct instances for the task.
It should be noted that in a larger cluster this behavior might differ, as the container may be restarted on any of the \textit{EC2} nodes available where the image required might not already exist.

\textit{Lambda} and \textit{fargate} have an additional built-in resilience factor, as replicas are automatically scheduled across different underlying hardware, situated across various \textit{AZs} in a \textit{region}.
Comparing this to \textit{EC2} workloads, which require specific configuration (and additional cost) to ensure that \textit{VMs} are spread across different \textit{AZs},
which still does not protect from an entire \textit{VM} level failure.

\textit{EKS} handles \textit{EC2} \textit{VM} level issues far quicker than \textit{ECS}. This can be attributed to a more-optimized controller-agent on the \textit{k8s} architecture when dealing with \textit{Virtual-Machines}.
\textit{ECS}, however easily takes the lead when it comes to scheduling \textit{fargate} workloads. This can be attributed to \textit{ECS} being built and designed around \textit{fargate} workloads, whilst \textit{EKS}
supports it only via adaptors and abstraction layers (which adds latency).


\subsection{Ease-of-Use}
\textit{Serverless} options have the clear advantage in amount of effort required to configure and create (by design), as a user does not need to
create, manage, or maintain underlying \textit{Virtual Machine} instances. The cost of this is in potential complexity required to get container workloads running on the
platform, as seen by \textit{Lambda} result.
\textit{ECS} and \textit{EKS} have a similar cost of effort to configure and create. \textit{ECS} has the advantage when comes to deploying workloads as it requires a
single JSON task-definition file, whilst a \textit{k8s} deployment may consist of multiple objects each requiring specific configuration.
This however would not be an issue for existing users of \textit{Kubernetes}.

\section{Conclusion}

This project aimed to compare \emph{Cloud Container Orchestration} platforms in respect to
\begin{itemize}
      \item ease of adoption and configuration
      \item deployment process
      \item restrictions and limitations
      \item performance
      \item cost impacts
      \item reliability and resilience
\end{itemize}

\noindent Based on the experimentation results presented and aforementioned argued points, the following conclusions are made
\begin{itemize}
      \item selection of container-orchestration platform will have a minor impact on performance on workloads
            \begin{itemize}
                  \item \textit{EKS} running workloads generally performed closer to the benchmark cluster across the widest array of tests,
                        with both \textit{ECS} followed by \textit{Lambda}, lagging constantly behind, with low but measurable performance impact.
            \end{itemize}
      \item selection of underlying server-architecture has a far more significant impact on performance for most workloads
            \begin{itemize}
                  \item \textit{EC2} based workloads consistently performed closer (or on occasion out-performed) the benchmark cluster,
                        whilst both \textit{Serverless} options (\textit{Fargate} and \textit{Lambda} respectively) producing lower performance,
                        even under the same orchestration platform
            \end{itemize}
      \item Network latency does not mask the loss in (measurable) performance between the various orchestration platforms or underlying server-architecture
            \begin{itemize}
                  \item Benchmarks showed a consistent degradation in performance between the tested platforms, irrespective of network latency.
            \end{itemize}
      \item Whilst \textit{Serverless Functions} may have the ability to run container-based workloads,
            the numerous limitations of attempting to run standard \textit{container-workloads} on a
            \textit{Serverless} architectural design, makes it an inviable replacement for an existing container-orchestration platform
      \item \textit{Cloud-Native} container orchestration platforms generally offer a more \emph{consistent} experience
            (in context to the rest of the chosen \textit{cloud} provider's environment), with simpler tie-ins to other offered services, and lower costs.
            These \emph{ease-of-use} factors are off-set by a measurable loss in performance, marginal (yet observable) lower level of resiliency, provider \textit{lock-in},
            and limited support.
      \item Managed Cloud \textit{Kubernetes} platforms benefit from utilizing an \textit{open-sourced}, \emph{ubiquitous} and near \textit{standard} platform
            that a \textit{k8s} cluster offers (this includes that existing tooling would require little-to-no changes as part of the migration process),
            at the cost of increased complexity (in terms of tie-ins to other cloud offered services and architectural design),
            and an increase in monthly costing.
\end{itemize}

\section{Future Work}
In respect of continuing research under this topic, the following points should be considered:
\begin{itemize}
      \item This project compared \emph{Cloud Container Orchestration} platforms under the lense of \emph{migration},
            an interesting lense would be a comparison under the lense of a fresh technological \emph{adoption} of \textit{containerization}
            (as this is the state most start-ups and newly founded enterprises find themselves in)
      \item \textit{AWS} and its selected representations of the various available technologies were used for experimentation in this project
            under the premise that the other cloud-providers offer services which are identical at a feature-parity level.
            Verification of this claim (running the various experiments across other cloud-providers) could illustrate hidden implementation costs
            (in terms of the previously mentioned comparison topics) may surface
      \item Due to the current restrictions of running \textit{container workloads} as \textit{Serverless Functions},
            all of the container-specific performance benchmarks were unable to complete successfully.
            This, therefore, restricted the ability to compare the \textit{Serverless} offering \emph{like-for-like} with its other counter-parts.
            Researching (or creating) benchmarking tools which are able to successfully run on \textit{Serverless} and \textit{container} workloads alike,
            would provide a deeper insight into the performance cost of \textit{Serverless Function} architecture.
\end{itemize}
