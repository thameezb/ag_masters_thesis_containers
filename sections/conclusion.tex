\part{Discussion, Conclusion and Future Work}

This chapter evaluates the aforementioned results in context of the posed problem.
Additionally concluding statements are presented, with potential future-work mentioned.

\chapter{Discussion}

\section{Performance and Latency}
Each platform was subjected to an identical set of broad benchmarks aiming to fully test the impact of both
selection of \emph{orchestration platform} and \emph{underlying architecture} in relation to performance.

\subsection{CPU}
CPU performance was evaluated by performing CPU intensive tasks, such as computing prime numbers using a prime sieve,
and archive compression/ decompression and measuring the time taken to complete the respective tasks.

Figure \ref{fig:perf_sysbench} illustrates the amount of time taken (s) to compute primes up to 1000000 across 4 threads using \textit{Sysbench}.
The \textit{EC2} instance of the \textit{EKS} cluster outperformed the baseline \textit{RKE} instance by a margin of ~2\%,
with the \textit{fargate} solution performing 20 \% slower than the baseline.
Finally, both the \textit{ECS} backed instances completed the task about 50\% slower than the baseline.

Figure \ref{fig:perf_7zip} illustrates the rating in million of instructions per second (MIPS) when performing the 7zip \textit{LZMA} compression and decompression benchmark.
Under this benchmark, the baseline \textit{RKE} instance posted the best result, with the two \textit{EC2} backed instances completing around 9\% (\textit{EKS}) and 10 \% (\textit{ECS}) less instructions respectively.
The two \textit{fargate} backed instances struggled immensely to complete this benchmark,
with the \textit{EKS} instance completing 80\% less tasks, and the \textit{ECS} instance completing 95 \% less tasks.

\noindent \newline It should be noted that due to runtime, and execution time-limit, limitations \textit{Lambda} did not complete in these CPU specific benchmarks.

\subsection{Memory}
Two criterion of memory performance was evaluated, the first being \emph{speed},
which was evaluated using the RAMSpeed, and \emph{bandwidth} using the MBW tool.

RAMSpeed performs four distinct memory intensive tasks with results, each measuring a different aspect of memory performance.
All results are recorded as a measurement of speed (MB/S).

This discussion will focus on last column in Figure \ref{fig:perf_RAMSpeed} as it illustrates the average speed recorded for the entirety of the benchmark (that is all four distinct tasks),
as the results (and performance difference) found in this table is wholly illustrative to the results found in each of the sub-tasks.
The \textit{EC2} backed \textit{EKS} instance outperformed the baseline \textit{RKE} instance by completing the tasks close to 36\% quicker.
This was followed by a significantly slower \textit{fargate} backed instance of \textit{EKS}, and \textit{EC2} backed \textit{ECS} instance,
which both completed close to 80\% slower than the baseline.
Finally the \textit{fargate} backed \textit{ECS} instance completed the benchmark close to 88\% slower.

\noindent \newline MBW measures the amount of copy memory bandwidth available to a user-application in terms of speed (MB/s).

Figure \ref{fig:perf_MBW} illustrates the baseline \textit{RKE} instance completed this benchmark with the fastest speed, with the \textit{EC2} backed \textit{EKS} instance following at 29\% slower.
The \textit{fargate} backed \textit{EKS} instance completed 64\% slower,
followed by the two \textit{ECS} workloads at 83\% and 87\% slower for the \textit{EC2} and \textit{fargate} instances respectively.

\noindent \newline It should be noted that due to runtime, and execution time-limit, limitations \textit{Lambda} did not complete in these Memory specific benchmarks.

\subsection{I/O}
fs\_mark evaluates a system's underlying file-system by performing heavily synchronous IO tasks across multiple folders/drives, measured in terms of speed (number of files per second).

Figure \ref{fig:perf_FSMark} illustrates the \textit{EC2} backed \textit{ECS} instance outperforming the baseline \textit{RKE} instance by close to 64\%,
followed closely by the \textit{EC2} backed \textit{EKS} instance which outperformed the baseline by close to 63\%.

\noindent \newline It should be noted that due to file-system mount limitations, neither of the \textit{Serverless} backed instances( that is \textit{Lambda} and \textit{fargate}) were able to complete these I\/O specific benchmarks.

\subsection{General Workloads}
In addition to focused hardware specific benchmarks (which are not often illustrative of real-world performance),
the following benchmarks were run to simulate performance under daily-tasks.

\subsubsection{m-queens}
m-queens solves the \emph{n-queens} problem using multi-threading, measured in terms of time-to-completion (s).

Figure \ref{fig:perf_mQueens} shows the baseline \textit{RKE} cluster complete in the quickest amount of time,
followed by the \textit{EC2} backed \textit{EKS} instance almost 50\% slower.
The other instances completed this task an order of magnitude slower, all completing close to 99\% slower than baseline.

\subsubsection{tool-container-benchmark}
\emph{tool-container-benchmark} simulates a general \textit{CRUD} based workload at load, measured in terms of time-to-completion (m).

Due to extreme network latency in terms of the database, the set of benchmarks were run twice,
first against an on-premise database, and the second against a cloud-hosted \textit{RDS} instance (illustrated by Figure \ref{fig:perf_tcb_default}).
Additionally in an attempt to cater for \textit{Lambda}, the set of benchmarks were re-run with a limit of 30000 \emph{events},
against both the on-premise and RDS databases
(illustrated by Figure \ref{fig:perf_tcb_30000}).

The \textit{RDS} series in Figure \ref{fig:perf_tcb_default} illustrates the \textit{EC2} backed \textit{EKS} instance performing the quickest,
followed by the two \textit{ECS} instances, with the \textit{fargate} backed and \textit{EC2} instances completing 9\% and 14\% slower respectively.
Finally, the \textit{fargate} backed \textit{EKS} instance completed the slowest (excluding the baseline \textit{RKE} instance).

The \textit{RDS} series in Figure \ref{fig:perf_tcb_30000} illustrates an anomaly with the \textit{fargate} backed \textit{ECS}
instance completing around 40\% quicker than the other instances, with all other instances completing within 30s of each other
except for \textit{Lambda} which completed close to 64\% slower.

Even with the extreme network latency caused by the database, a clear performance difference is seen by the \textit{On-premise} series in both Figures \ref{fig:perf_tcb_default} and Figures \ref{fig:perf_tcb_30000}
when comparing between the various instances (excluding the baseline instance).

Figure \ref{fig:perf_tcb_network} compares the results of each instance with the database located closest to it (in an effort to remove network latency as a factor of comparison) with \textit{number-of-events} set to 30000,
which continues to illustrate the pattern of \textit{EKS} performing better than \textit{ECS},
and \textit{EC2} backed instances performing better than their \textit{fargate} backed instances.


\subsection*{Conclusion}
As illustrated above a near constant pattern can be seen across each of the benchmarks run. \textit{EKS} workloads performed closest (and occasionally outperformed) the baseline \textit{RKE} instances,
with \textit{ECS} workloads lagging behind. This can be attributed to the hidden overhead by the managing agents and platform architecture of \textit{ECS}.
Being a \emph{proprietary} and \emph{closed} technology, all performance optimization is done \emph{in-house} by the cloud-provider.
Comparing this to \textit{Kubernetes}, being an \textit{open-source} technology, it is worked on by over 43 000 (as of 2020)\cite{pittman_2020} of contributors, which would contribute towards its consistently better optimized
performance. This would be applicable to features like \emph{burstable} resource limits (which allows a container instance to consume above its resource request for short-periods of time) as well.

Additionally it is noted that \textit{Serverless} (\textit{fargate} and \textit{Lambda})
architectures showed significant performance drops when compared to their \textit{Virtual Machine} (EC2) counterparts,
even when managed by the same orchestration platform.
A potential reasoning for this would be the extra network-hops required for these \textit{Serverless}
instances to communicate with the platform managing-agents and other resources within a customer \textit{VPC}.
Finally, \textit{Serverless} platforms run on shared tenant hardware managed by the cloud provider,
which would in turn be stricter on the resource usage per container instance.

\section{Cost}
Figure \ref{fig:cost_workload} illustrates the difference in average costing when running \emph{tool-container-benchmark} three times with 30000 events against an \textit{RDS} instance, measured in US Dollars.
This table includes the dollar costing of running the actual workload (for the given period) and the cost of all dependencies,
this may include the environment or required additional services like an \textit{ALB}, over a given period.
The \textit{Serverless} instances have the advantage with \textit{Lambda} costing the lowest amount, followed by the \textit{fargate} backed \textit{ECS} and \textit{EKS} instances.
The two \textit{EC2} instances follow with \textit{ECS} and \textit{EKS} respectively.

Table \ref{fig:cost_projected} projects the costing of running a long-lived services on each platform. Here we see the two \textit{ECS} backed instances taking the lead,
with two \textit{EKS} backed instances following. For both of the above results, when scaling the cost of an \textit{EC2} instance to a cost per container
(as a single \textit{VM} can run 32 containers of the tested spec at the same cost), \textit{fargate} begins to exceed the cost of \textit{EC2}.
Additionally the \textit{EKS} platform cost is per cluster (not per container workload), therefore the additional \$73 cost per month of \textit{EKS} needs to be seen in the context of the amount of workloads being run on a cluster.
It should be noted that due to \textit{Lambda}'s time-limitation of 15 minutes, the value in this table is a theoretical project based on cost per second.

\subsection*{Conclusion}
For short lived or burst-able workloads, \textit{Serverless} options have the clear advantage as there are no additional environmental costs to consider.
However, as noted by TODO-addRef and the results of forecast in Table\ref{fig:cost_projected}, the longer the workload needs to run, the more cost-effective \textit{EC2} instances become.
Whilst \textit{ECS} and \textit{EKS} cost the same from a pure workload perspective, the additional monthly cost of running an \textit{EKS} cluster does become a contributing factor
when comparing platforms at a pure dollar cost.

\section{Resilience and Reliability}
Each platform was subjected to \textit{Chaos-Monkey} style tests to assert a level of resilience and reliability under extreme circumstances.

Figure \ref{fig:rr_scaling} illustrates the average amount of \textit{downtime} when scaling down the number of container-workloads to 0 and back to 1, measured in seconds.
The two \textit{EC2} backed workloads completed the scaling in the quickest, with \textit{EKS} completing first, followed by \textit{ECS}.
The two \textit{fargate} workloads followed, with \textit{ECS} completing 33\% quicker than \textit{EKS}.
It should be noted that due to inability to scale \textit{Lamda} functions, it did not participate in this test.

Figure \ref{fig:rr_deleteContainer} illustrates the average amount of time taken between a container workload being forcibly killed, and the workload being restarted, and ready to respond.
\textit{EKS} once again responded to this 35\% quicker than the \textit{ECS} workload, where the controller still noted the removed container as active and ready.
It should be noted that due to the \textit{Serverless} nature of both \textit{Lambda} and \textit{fargate}, neither of these participated in this test.

Figure \ref{fig:rr_reboot} illustrates the average amount of time taken between an underlying host being rebooted unexpectedly, the workload being restarted, and ready to respond.
\textit{ECS} handled the scale-in process 17\% faster than its \textit{EKS} counter-part.
It should be noted that due to the \textit{Serverless} nature of both \textit{Lambda} and \textit{fargate}, neither of these participated in this test.

\subsection*{Conclusion}
\textit{EC2} instances have the distinct speed advantage when restarting unhealthy containers.
This can be attributed to the container images already being present on the nodes required, whereas \textit{fargate} starts a purely distinct instances for the task.
It should be noted that in a larger cluster this behavior might differ, as the container may be restarted on any of the \textit{EC2} nodes available where the image required might not already exist.

\textit{Lambda} and \textit{fargate} have an additional built-in resilience factor, as replicas are automatically scheduled across different underlying hardware, situated across various \textit{AZs} in a \textit{region}.
Comparing this to \textit{EC2} workloads, which require specific configuration (and additional cost) to ensure that \textit{VMs} are spread across different \textit{AZs},
which still does not protect from an entire \textit{VM} level failure.

\textit{EKS} handles \textit{EC2} \textit{VM} level issues far quicker than \textit{ECS}. This can be attributed to a more-optimized controller-agent on the \textit{k8s} architecture when dealing with \textit{Virtual-Machines}.
\textit{ECS}, however easily takes the lead when it comes to scheduling \textit{fargate} workloads. This can be attributed to \textit{ECS} being built and designed around \textit{fargate} workloads, whilst \textit{EKS}
supports it only via adaptors and abstraction layers (which adds latency).


\section{Ease-of-Use}
Each platform chosen required an architectural design, object configuration, and environment deployment.
In addition, once every environment was ready, a deployment process needed to be designed to run our test workloads on the selected platforms.

\subsection{Lambda}
\textit{Lambda} proved to be the easiest environment to get configured, as there was no real "environment" to create. Being a serverless function,
to run, it required that a Docker image exist (at this moment restricted to \textit{ECR} repositories only with a max image size of 10GB), environment variables to exist and be configured, max amount of memory
and a maximum timeout (being less than 15 minutes). This however, came at an extreme cost of flexibility and configuration.
\textit{Lambdas} being serverless \emph{functions} meant that the expected workload was a function (that is to perform a single task quickly which accepted a specific payload and response),
meant that most of the selected benchmarks were incompatible with the underlying architecture (due to the max size of 10GB of ephemeral storage available).
Where it was possible to run, the code had to be altered significantly and cater for specific \textit{Lambda} requirements.

Deployments of workloads occur via the AWS API (using the CLI, Web-Console or other tools like Terraform)

\subsection{ECS}
\textit{ECS} being a cloud-native \textit{orchestration} proved fairly simple to create and configure.
A \emph{cluster} needed to be created (which simply required a unique name), with \textit{capacity-providers} being registered to the cluster,
which can be of type \textit{fargate} or \textit{EC2}.
This process occurs almost immediately, and workloads can be scheduled once a providers is registered.
Workloads are defined using a \textit{task-definition}, a JSON file which describes the intended container-workloads.
Ingress connectivity would be allowed via an external \textit{ALB} which plugs directly into the rest of the architecture.

Running workloads on \textit{fargate}, being \textit{serverless} took little to no effort, as no additional resources needed to be created or configured.
Running workloads on \textit{EC2}, required far more effort, as \textit{AMI}'s needed to be built with the required \textit{ECS} agent (and dependencies) installed, tested,
and configured. Additionally \textit{ASG}'s needed to be configured and registered with the \textit{ECS} cluster before workloads would be scheduled.

Deployments of workloads occur via the AWS API (using the CLI, Web-Console or other tools like Terraform)

\subsection{EKS}
A bare \textit{EKS} required a similar amount of effort to create, configure as \textit{ECS}. A \emph{cluster} is created with basic configuration requirements,
further add-ons are then installed separately to add additional features required to get a \textit{k8s} cluster working in the cloud environment.
This process can take up to 30 minutes to complete.
\textit{Worker} instances are then registered to the \textit{EKS} cluster (either \textit{EC2} or \textit{fargate}).
Workloads are defined using using standard \textit{k8s} YAML object files (depending on the object type desired).
For users unfamiliar with the \textit{k8s} architecture, this can be a source of great complexity and new tooling requirements.
Ingress connectivity requires an additional ingress-controller component to be deployed into the cluster, coupled with an \textit{ALB}.

Running workloads on \textit{fargate} and \textit{EC2} is quite similar in complexity to \textit{ECS}.

Deployments of workloads occur via a specific AWS tool called \emph{eksctl}\cite{weaveworks} or the standard \textit{k8s} deployment tool \emph{kubectl}\cite{kubernetes}

\subsection*{Conclusion}
Figure \ref{fig:eou} illustrates the amount of time taken to bootstrap and configure each platform to be ready to run workloads (measured in minutes).
and the amount of time required to convert an existing container workload to run on each platform (measured in minutes).
The above table illustrates that \textit{Serverless} options have the clear advantage in amount of effort required to configure and create (by design), as a user does not need to
create, manage, or maintain underlying \textit{Virtual Machine} instances. The cost of this is in potential complexity required to get container workloads running on the
platform, as seen by \textit{Lambda} result.
\textit{ECS} and \textit{EKS} have a similar cost of effort to configure and create. \textit{ECS} has the advantage when comes to deploying workloads as it requires a
single JSON task-definition file, whilst a \textit{k8s} deployment may consist of multiple objects each requiring specific configuration.
This however would not be an issue for existing users of \textit{Kubernetes}.

\chapter{Conclusion}

This project aimed to compare \emph{Cloud Container Orchestration} platforms in respect to
\begin{itemize}
      \item ease of adoption and configuration
      \item deployment process
      \item restrictions and limitations
      \item performance
      \item cost impacts
      \item reliability and resilience
\end{itemize}

\noindent Based on the experimentation results presented and aforementioned argued points, the following conclusions are made
\begin{itemize}
      \item selection of container-orchestration platform will have a minor impact on performance on workloads
            \begin{itemize}
                  \item \textit{EKS} running workloads generally performed closer to the benchmark cluster across the widest array of tests,
                        with both \textit{ECS}, followed by \textit{Lambda}, lagging constantly behind, with low, but measurable performance impact.
            \end{itemize}
      \item selection of underlying server-architecture has a far more significant impact on performance for most workloads
            \begin{itemize}
                  \item \textit{EC2} based workloads consistently performed closer (or on occasion out-performed) our benchmark cluster,
                        whilst both \textit{Serverless} options (\textit{Fargate} and \textit{Lambda} respectively) producing lower performance,
                        even under the same orchestration platform
            \end{itemize}
      \item Network latency does not mask the loss in (measurable) performance between the various orchestration platforms or underlying server-architecture
            \begin{itemize}
                  \item Benchmarks showed a consistent degradation in performance between the tested platforms, irrespective of network latency.
            \end{itemize}
      \item Whilst \textit{Serverless Functions} may have the ability to run container-based workloads,
            the numerous limitations of attempting to run standard \textit{container-workloads} on a
            \textit{Serverless} architectural design, makes it an inviable replacement for an existing container-orchestration platform
      \item \textit{Cloud-Native} container orchestration platforms generally offer a more \emph{consistent} experience
            (in context to the rest of the chosen \textit{cloud} provider's environment), with simpler tie-ins to other offered services, and lower costs.
            These \emph{ease-of-use} factors are off-set by a measurable loss in performance, marginal (yet observable) lower level of resiliency, provider \textit{lock-in},
            and limited support.
      \item Managed Cloud \textit{Kubernetes} platorms benefit from utilizing an \textit{open-sourced}, \emph{ubiquitos} and near \textit{standard} platform
            that a \textit{k8s} cluster offers (this includes that existing tooling would require little-to-no changes as part of the migration process),
            at the cost of increased complexity (in terms of tie-ins to other cloud offered services and architectural design),
            and an increase in monthly costing.
\end{itemize}

\chapter{Future Work}
In respect of continuing research under this topic, the following points should be considered:
\begin{itemize}
      \item This project compared \emph{Cloud Container Orchestration} platforms under the lense of \emph{migration},
            an interesting lense would be a comparison under the lense of a fresh technological \emph{adoption} of \textit{containerization}
            (as this is the state most start-ups and newly founded enterprises find themselves in)
      \item \textit{AWS} and its selected representations of the various available technologies were used for experimentation in this project
            under the premise that the other cloud-providers offer services which are identical at a feature-parity level.
            Verification of this claim (running the various experiments across other cloud-providers) could illustrate hidden implementation costs
            (in terms of the previously mentioned comparison topics) may surface
      \item Due to the current restrictions of running \textit{container workloads} as \textit{Serverless Functions},
            all of the container-specific performance benchmarks were unable to complete successfully.
            This, therefore, restricted the ability to compare the \textit{Serverless} offering \emph{like-for-like} with its other counter-parts.
            Researching (or creating) benchmarking tools which are able to successfully run on \textit{Serverless} and \textit{container} workloads alike,
            would provide a deeper insight into the performance cost of \textit{Serverless Function} architecture.
\end{itemize}
