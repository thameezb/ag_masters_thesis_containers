\part{Discussion, Conclusion and Future Work}

This chapter evaluates the aforementioned results in context of the posed problem.
Additionally concluding statements are presented, with potential future-work mentioned.

\chapter{Discussion}

\section{Performance and Latency}
Each platform was subjected to an identical set of broad benchmarks aiming to fully test the impact of both
selection of \emph{orchestration platform} and \emph{underlying architecture} in relation to performance.

\subsection{CPU}
CPU performance was evaluated by performing CPU intensive tasks
(such as computing prime numbers using a prime sieve and archive compression/decompression)
and measuring the time taken to complete the respective tasks.
\indent Table TODO-addRef illustrates the amount of time taken (s) to compute primes up to 1000000 across 4 threads.
The \textit{EC2} instance of the \textit{EKS} cluster outperformed the baseline \textit{RKE} instance by a margin of ~2\%,
with the \textit{fargate} solution performing 20 \% slower than the baseline.
Finally, both the \textit{ECS} backed instances completed the task about 50\% slower than the baseline.

Table TODO-addRef and TODO-addRef illustrates the a rating in million of instructions per second (MIPS) when performing the 7zip \textit{LZMA} compression and decompression benchmark respectively.
Under this benchmark, the baseline \textit{RKE} instance posted the best result, with the two \textit{EC2} backed instances completing around 9\% (\textit{EKS}) and 10 \% (\textit{ECS}) less instructions respectively.
The two \textit{fargate} backed instances struggled immensely to complete this benchmark,
with the \textit{EKS} instance completing 80\% less tasks, and the \textit{ECS} instance completing 95 \% less tasks.

\noindent \newline It should be noted that due to runtime, and execution time-limit, limitations \textit{Lambda} did not complete in these CPU specific benchmarks.

\subsection{Memory}
Two criterion of memory performance was evaluated, the first being \emph{speed},
which was evaluated using the RAMSpeed tool TODO-addRef, and \emph{bandwidth} using the MBW tool TODO-addRef.

RAMSpeed performs four distinct memory intensive tasks with results, each measuring a different aspect of memory performance.
All results are recorded as a measurement of speed (MB/S).
This discussion will focus on Table TODO-addRef as it illustrates the average speed recorded for the entirety of the benchmark (that is all four distinct tasks),
as the results (and performance difference) found in this table is wholly illustrative to the results found in each of the sub-tasks.
The \textit{EC2} backed \textit{EKS} instance outperformed the baseline \textit{RKE} instance by completing the tasks close to 36\% quicker.
This was followed by a significantly slower \textit{fargate} backed instance of \textit{EKS}, and \textit{EC2} backed \textit{ECS} instance,
which both completed close to 80\% slower than the baseline.
Finally the \textit{fargate} backed \textit{ECS} instance completed the benchmark close to 88\% slower.

\noindent \newline MBW measures the amount of copy memory bandwidth available to a user-application in terms of speed (MiB/s).
The baseline \textit{RKE} instance completed this benchmark the quickest, with the \textit{EC2} backed \textit{EKS} instance following at 29\% slower.
The \textit{fargate} backed \textit{EKS} instance completed 64\% slower,
followed by the two \textit{ECS} workloads at 83\% and 87\% slower for the \textit{EC2} and \textit{fargate} instances respectively.

\noindent \newline It should be noted that due to runtime, and execution time-limit, limitations \textit{Lambda} did not complete in these Memory specific benchmarks.

\subsection{I/O}
fs\_mark TODO-addRef evaluates a system's underlying file-system by performing heavily synchronous IO tasks across multiple folders\/drives, measured in terms of speed (number of files per second).
Table TODO-addRef shows the \textit{EC2} backed \textit{ECS} instance outperforming the baseline \textit{RKE} instance by close to 64\%,
followed closely by the \textit{EC2} backed \textit{EKS} instance which outperformed the baseline by close to 63\%.

\noindent \newline It should be noted that due to file-system mount limitations, neither of the \textit{Serverless} backed instances( that is \textit{Lambda} and \textit{fargate}) were able to complete these I\/O specific benchmarks.

\subsection{General Workloads}
In addition to focused hardware specific benchmarks (which are not often illustrative of real-world performance),
the following benchmarks were run to simulate performance under daily-tasks.

\subsubsection{m-queens}
m-queens TODO-addRef solves the \emph{n-queens} TODO-addRef problem using multi-threading, measured in terms of time-to-completion (s).
Table TODO-addRef shows the baseline \textit{RKE} cluster complete in the quickest amount of time,
followed by the \textit{EC2} backed \textit{EKS} instance almost 50\% slower.
The other instances completed this task an order of magnitude slower, all completing close to 99\% slower than baseline.

\subsubsection{tool-container-benchmark}
\emph{tool-container-benchmark} simulates a general \textit{CRUD} based workload at load, measured in terms of time-to-completion (m).

Due to extreme network latency in terms of the database, the set of benchmarks were run twice, first against an on-premise database (illustrated by Table TODO-addRef),
and the second against a cloud-hosted \textit{RDS} instance (illustrated by Table TODO-addRef).
Additionally in an attempt to cater for \textit{Lambda}, the set of benchmarks were run with a limit of 30000 \emph{events}, against both the on-premise and RDS databases
(illustrated by Table TODO-addRef and Table TODO-addRef) respectively

Even with the extreme network latency caused by the database, a clear performance difference is seen in Table TODO-addRef and TODO-addRef when comparing between the various instances (excluding the baseline instance),
with the \textit{EC2} backed \textit{EKS} instance performing consistently the quickest, followed by the \textit{fargate} backed \textit{EKS} instance,
with the two \textit{EC2} and \textit{fargate} backed \textit{ECS} instances completing third and forth, with the \textit{Lambda} instance completing last.

Table TODO-addRef illustrates a different pattern (once again excluding the baseline) with the \textit{EC2} backed \textit{EKS} instance performing the quickest,
followed by the two \textit{ECS} instances, with the \textit{fargate} backed and \textit{EC2} instances completing 9\% and 14\% slower respectively.
Finally, the \textit{fargate} backed \textit{EKS} instance completed 18\% slower.

Table TODO-addRef illustrates an anomaly with the \textit{fargate} backed \textit{ECS} instance completing around 40\% quicker than the other instances, with all other instances completing within 30s of each other
except for \textit{Lambda} which completed close to 64\% slower.

Finally Table TODO-addRef compares the results of each instance with the database located closest to it (in an effort to remove network latency as a factor of comparison).

\subsection{Conclusion}
As illustrated above a near constant pattern can be seen across each of the benchmarks run. \textit{EKS} workloads performed closest (and occasionally outperformed) the baseline \textit{RKE} instances,
with \textit{ECS} workloads lagging behind. This can be attributed to the hidden overhead by the managing agents and platform architecture of \textit{ECS}.
Being a \emph{proprietary} and \emph{closed} technology, all performance optimization is done \emph{in-house} by the cloud-provider.
Comparing this to \textit{Kubernetes}, being an \textit{open-source} technology, it is worked on by millions of contributors TODO-addRef, which would contribute towards its consistently better optimized
performance. This would be applicable to features like \emph{burstable} resource limits (which allows a container instance to consume above its resource request for short-periods of time) as well.

Additionally it is noted that \textit{Serverless} (\textit{fargate} and \textit{Lambda})
architectures showed significant performance drops when compared to their \textit{Virtual Machine} (EC2) counterparts,
even when managed by the same orchestration platform.
A potential reasoning for this would be the extra network-hops required for these \textit{Serverless}
instances to communicate with the platform managing-agents and other resources within a customer \textit{VPC}.
Finally, \textit{Serverless} platforms run on shared tenant hardware managed by the cloud provider,
which would in turn be stricter on the resource usage per container instance.

\section{Cost}

\section{Ease-of-Use}
Each selected platform chosen required an architectural design, object configuration, and environment deployment.
In addition, once every environment was ready, a deployment process needed to be designed to run our test workloads on the selected platforms.

\subsection{Lambda}
\textit{Lambda} proved to be the easiest environment to get configured, as there was no real "environment" to create. Being a serverless function,
to run, it required that a Docker image exist (at this moment restricted to \textit{ECR} repositories only with a max image size of 10GB), environment variables to exist and be configured, max amount of memory
and a maximum timeout (being less than 15 minutes). This however, came at an extreme cost of flexibility and configuration.
\textit{Lambdas} being serverless \emph{functions} meant that the expected workload was a function (that is to perform a single task quickly which accepted a specific payload and response),
meant that most of the selected benchmarks were incompatible with the underlying architecture (due to the max size of 10GB of ephemeral storage available).
Where it was possible to run, the code had to be altered significantly and cater for specific \textit{Lambda} requirements.

Deployments of workloads occur via the AWS API (using the CLI, Web-Console or other tools like Terraform)

\subsection{ECS}
\textit{ECS} being a cloud-native \textit{orchestration} proved fairly simple to create and configure.
A \emph{cluster} needed to be created (which simply required a unique name), with \textit{capacity-providers} being registered to the cluster,
which can be of type \textit{fargate} or \textit{EC2}.
This process occurs almost immediately, and workloads can be scheduled once a providers is registered.
Workloads are defined using a \textit{task-definition}, a JSON file which describes the intended container-workloads.
Ingress connectivity would be allowed via an external \textit{ALB} which plugs directly into the rest of the architecture.

Running workloads on \textit{fargate}, being \textit{serverless} took little to no effort, as no additional resources needed to be created or configured.
Running workloads on \textit{EC2}, required far more effort, as \textit{AMI}'s needed to be built with the required \textit{ECS} agent (and dependencies) installed, tested,
and configured. Additionally \textit{ASG}'s needed to be configured and registered with the \textit{ECS} cluster before workloads would be scheduled.

Deployments of workloads occur via the AWS API (using the CLI, Web-Console or other tools like Terraform)

\subsection{EKS}
A bare \textit{EKS} required a similar amount of effort to create, configure as \textit{ECS}. A \emph{cluster} is created with basic configuration requirements,
further add-ons are then installed separately to add additional features required to get a \textit{k8s} cluster working in the cloud environment.
This process can take up to 30 minutes to complete.
\textit{Worker} instances are then registered to the \textit{EKS} cluster (either \textit{EC2} or \textit{fargate}).
Workloads are defined using using standard \textit{k8s} YAML object files (depending on the object type desired).
For users unfamiliar with the \textit{k8s} architecture, this can be a source of great complexity and new tooling requirements.
Ingress connectivity requires an additional ingress-controller component to be deployed into the cluster, coupled with an \textit{ALB}.

Running workloads on \textit{fargate} and \textit{EC2} is quite similar in complexity to \textit{ECS}.

Deployments of workloads occur via a specific AWS tool called \emph{eksctl} TODO-addRef or the standard \textit{k8s} deployment tool \emph{kubectl} TODO-addRef

\subsection{Conclusion}
Table TODO-addRef illustrates the amount of time taken to bootstrap and configure each platform to be ready to run workloads.
Table TODO-addRef illustrates the amount of time required to convert an existing container workloads to run on each platform.
\textit{Serverless} options have the clear advantage in amount of effort required to configure and create (by design), as a user does not need to
create, manage or maintain underlying \textit{Virtual Machine} instances. The cost of this is potential complexity required to get container workloads running on the
platform, as seen by \textit{Lambda}.
\textit{ECS} and \textit{EKS} have a similar cost of effort to configure and create. \textit{ECS} has the advantage when comes to deploying workloads as it requires a
single JSON task-definition file, whilst a \textit{k8s} deployment may consist of multiple objects each requiring specific configuration.
This however would not be an issue for existing users of \textit{Kubernetes}.

\section{Resilience and Reliability}


\chapter{Conclusion and Future Work}

This project aimed to compare \emph{Cloud Container Orchestration} platforms in respect to
\begin{itemize}
      \item ease of adoption and configuration
      \item deployment process
      \item restrictions and limitations
      \item performance
      \item cost impacts
      \item reliability and resilience
\end{itemize}

\noindent Based on the experimentation results presented and aforementioned argued points, the following conclusions are made
\begin{itemize}
      \item selection of container-orchestration platform will have a minor impact on performance on workloads
            \begin{itemize}
                  \item \textit{EKS} running workloads generally performed closer to the benchmark cluster across the widest array of tests,
                        with both \textit{ECS}, followed by \textit{Lambda}, lagging constantly behind, with low, but measurable performance impact.
            \end{itemize}
      \item selection of underlying server-architecture has a far more significant impact on performance for most workloads
            \begin{itemize}
                  \item \textit{EC2} based workloads consistently performed closer (or on occasion out-performed) our benchmark cluster,
                        whilst both \textit{Serverless} options (\textit{Fargate} and \textit{Lambda} respectively) producing lower performance,
                        even under the same orchestration platform
            \end{itemize}
      \item Network latency does not mask the loss in (measurable) performance between the various orchestration platforms or underlying server-architecture
            \begin{itemize}
                  \item Benchmarks showed a consistent degradation in performance between the tested platforms, irrespective of network latency.
            \end{itemize}
      \item Whilst \textit{Serverless Functions} may have the ability to run container-based workloads,
            the numerous limitations of attempting to run standard \textit{container-workloads} on a
            \textit{Serverless} architectural design, makes it an inviable replacement for an existing container-orchestration platform
      \item \textit{Cloud-Native} container orchestration platforms generally offer a more \emph{consistent} experience
            (in context to the rest of the chosen \textit{cloud} provider's environment), with simpler tie-ins to other offered services, and marginally lower costs.
            These \emph{ease-of-use} factors are off-set by a measurable loss in performance, marginal (yet observable) lower level of resiliency, provider \textit{lock-in},
            and limited support.
      \item Managed Cloud \textit{Kubernetes} platforms benefit from utilizing an \textit{open-sourced}, \emph{ubiquitos} and near \textit{standard} platform
            that a \textit{k8s} cluster offers (this includes that existing tooling would require little-to-no changes as part of the migration process),
            at the cost of increased complexity (in terms of tie-ins to other cloud offered services), and an increase in monthly costing.
\end{itemize}

\section{Future Work}
In respect of continuing research under this topic, the following points should be considered:
\begin{itemize}
      \item This project compared \emph{Cloud Container Orchestration} platforms under the lense of migration,
            an interesting lense would be a comparison under the lense of a fresh technological adoption of \textit{containerization}
            (as this is the state most start-ups and newly founded enterprises find themselves in)
      \item \textit{AWS} and its selected representations of the various available technologies were used for experimentation in this project
            under the premise that the other cloud-providers offer services which are identical at a feature-parity level.
            Verification of this claim (running the various experiments across other cloud-providers) could illustrate hidden implementation costs
            (in terms of the previously mentioned comparison topics) may surface
      \item Due to the current restrictions of running \textit{container workloads} as \textit{Serverless Functions},
            all of the container-specific performance benchmarks were unable to complete successfully.
            This, therefore, restricted the ability to compare the \textit{Serverless} offering \emph{like-for-like} with its other counter-parts.
            Researching (or creating) benchmarking tools which are able to successfully run on \textit{Serverless} and \textit{container} workloads alike,
            would provide a deeper insight into the performance cost of \textit{Serverless Function} architecture.
\end{itemize}
